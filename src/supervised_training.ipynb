{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from simulator import Sim\n",
    "import numpy as np\n",
    "import rospy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rospy.init_node('trainer', anonymous=True)\n",
    "sim = Sim()\n",
    "data = sim.states\n",
    "label = sim.actions\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "s_data, s_label = randomize(data,label)\n",
    "\n",
    "\n",
    "train_data = s_data[:,:].reshape(-1,12).astype(np.float32)\n",
    "train_label = s_label[:800,:].reshape(-1,3).astype(np.float32)\n",
    "test_data = s_data[800:,:].reshape(-1,12).astype(np.float32)\n",
    "test_label = s_label[800:,:].reshape(-1,3).astype(np.float32)\n",
    "\n",
    "train_data = train_data[:,9:]\n",
    "test_data = test_data[:,9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = pickle.load(open('./data/sim_data.p', 'rb'))\n",
    "data = np.array(sim_data['poses'])\n",
    "label = np.array(sim_data['actions'])\n",
    "s_data, s_label = randomize(data,label)\n",
    "\n",
    "\n",
    "train_data = s_data[:6500,:].reshape(-1,12).astype(np.float32)\n",
    "train_label = s_label[:6500,:].reshape(-1,3).astype(np.float32)\n",
    "valid_data = s_data[6500:7500,:].reshape(-1,12).astype(np.float32)\n",
    "valid_label = s_label[6500:7500,:].reshape(-1,3).astype(np.float32)\n",
    "test_data = s_data[7500:,:].reshape(-1,12).astype(np.float32)\n",
    "test_label = s_label[7500:,:].reshape(-1,3).astype(np.float32)\n",
    "\n",
    "train_data = train_data[:,9:]\n",
    "valid_data = valid_data[:,9:]\n",
    "test_data = test_data[:,9:]\n",
    "input_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "alpha = 0.01\n",
    "hid_num1 = 500\n",
    "hid_num2 = 100\n",
    "input_num = 3\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, input_num))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "    tf_valid_dataset = tf.constant(valid_data)\n",
    "    tf_valid_labels = tf.constant(valid_label)\n",
    "    tf_test_dataset = tf.constant(test_data)\n",
    "    tf_test_labels = tf.constant(test_label)\n",
    "    \n",
    "    F1_weights = tf.Variable(tf.truncated_normal([input_num,hid_num1], stddev=10.0))\n",
    "    F1_biases = tf.Variable(tf.constant(1.0, shape=[hid_num1]))\n",
    "    \n",
    "    F2_weights = tf.Variable(tf.truncated_normal([hid_num1, hid_num2], stddev=5.0))\n",
    "    F2_biases = tf.Variable(tf.constant(1.0, shape=[hid_num2]))\n",
    "    \n",
    "    F3_weights = tf.Variable(tf.truncated_normal([hid_num2,3], stddev=1.0))\n",
    "    F3_biases = tf.Variable(tf.constant(1.0, shape=[1]))\n",
    "    \n",
    "    def model(data):\n",
    "        fc = tf.matmul(data, F1_weights)\n",
    "        hidden = tf.nn.relu(fc + F1_biases)\n",
    "        \n",
    "        fc = tf.matmul(hidden, F2_weights)\n",
    "        hidden = tf.nn.sigmoid(fc + F2_biases)\n",
    "        \n",
    "        fc = tf.matmul(hidden, F3_weights)\n",
    "        output = fc + F3_biases\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    train_pred = model(tf_train_dataset)\n",
    "    reg_loss = alpha * (tf.nn.l2_loss(F1_weights) + tf.nn.l2_loss(F2_weights) + tf.nn.l2_loss(F3_weights))\n",
    "    loss = tf.losses.mean_squared_error(labels=tf_train_labels, predictions=train_pred)\n",
    "    loss1 = loss + reg_loss\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss1)\n",
    "    #optimizer = tf.train.RMSPropOptimizer(0.001).minimize(loss)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "    \n",
    "    valid_pred = model(tf_valid_dataset)\n",
    "    valid_loss = tf.losses.mean_squared_error(labels=tf_valid_labels, predictions=valid_pred)\n",
    "    test_pred = model(tf_test_dataset)\n",
    "    test_loss = tf.losses.mean_squared_error(labels=tf_test_labels, predictions=test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch training loss at step 0: 96.165932\n",
      "Minibatch validation loss at step 0: 100.676117\n",
      "--------------------------------------\n",
      "Minibatch training loss at step 2000: 16.612602\n",
      "Minibatch validation loss at step 2000: 19.108631\n",
      "--------------------------------------\n",
      "Minibatch training loss at step 4000: 6.392917\n",
      "Minibatch validation loss at step 4000: 12.422592\n",
      "--------------------------------------\n",
      "Minibatch training loss at step 6000: 4.166601\n",
      "Minibatch validation loss at step 6000: 11.064553\n",
      "--------------------------------------\n",
      "Minibatch training loss at step 8000: 5.942088\n",
      "Minibatch validation loss at step 8000: 8.986403\n",
      "--------------------------------------\n",
      "Minibatch training loss at step 10000: 2.756294\n",
      "Minibatch validation loss at step 10000: 4.264507\n",
      "--------------------------------------\n",
      "Minibatch training loss at step 12000: 3.605351\n",
      "Minibatch validation loss at step 12000: 1.926659\n",
      "--------------------------------------\n",
      "Minibatch training loss at step 14000: 1.830730\n",
      "Minibatch validation loss at step 14000: 1.006796\n",
      "--------------------------------------\n",
      "Test loss: 1.071\n",
      "Input an index of test image (or Enter to quit): 2\n",
      "[6.0, 17.0, 11.0]\n",
      "[  5.01774979  17.30291176  10.29197311]\n",
      "Input an index of test image (or Enter to quit): 3\n",
      "[3.0, 16.0, 9.0]\n",
      "[  3.19191885  15.66377354   8.37397671]\n",
      "Input an index of test image (or Enter to quit): 4\n",
      "[16.0, 13.0, 2.0]\n",
      "[ 16.07240868  12.36136627   2.15985942]\n",
      "Input an index of test image (or Enter to quit): 2\n",
      "[6.0, 17.0, 11.0]\n",
      "[  5.01774979  17.30291176  10.29197311]\n",
      "Input an index of test image (or Enter to quit): 2\n",
      "[6.0, 17.0, 11.0]\n",
      "[  5.01774979  17.30291176  10.29197311]\n",
      "Input an index of test image (or Enter to quit): 34\n",
      "[17.0, 17.0, 20.0]\n",
      "[ 15.92214012  14.74163151  17.81001472]\n",
      "Input an index of test image (or Enter to quit): 24\n",
      "[3.0, 16.0, 6.0]\n",
      "[  3.31577659  15.52440071   5.63142204]\n",
      "Input an index of test image (or Enter to quit): 52\n",
      "[4.0, 7.0, 6.0]\n",
      "[ 4.15541172  5.07568312  5.39348602]\n",
      "Input an index of test image (or Enter to quit): \n"
     ]
    }
   ],
   "source": [
    "num_steps = 15000\n",
    "batch_size = 10\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "with tf.Session(graph=graph, config = config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, vl = session.run([optimizer, loss, valid_loss], feed_dict=feed_dict)\n",
    "        if (step % 2000 == 0):\n",
    "            print('Minibatch training loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch validation loss at step %d: %f' % (step, vl))\n",
    "            print('--------------------------------------')\n",
    "    print('Test loss: %.3f' % test_loss.eval())\n",
    "    test_rslt = test_pred.eval()\n",
    "    i_test = 0\n",
    "    while(i_test!=''):\n",
    "        try:\n",
    "            i_test = input(\"Input an index of test image (or Enter to quit): \")\n",
    "            label = test_label[int(i_test),:].tolist()\n",
    "            rslt = test_rslt[i_test,:]\n",
    "            print label\n",
    "            print rslt\n",
    "        except:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
